{
  "id": "tensor-memory-management-kernel-innovation",
  "title": "Deep Dive: Tensor Memory Management - The Kernel Innovation Behind MLOS Performance",
  "date": "2026-01-05",
  "author": "MLOS Kernel Team",
  "category": "kernel",
  "featured": true,
  "badge": "PATENT",
  "summary": "A comprehensive technical exploration of MLOS's patented Tensor Memory Management system: Tensor Fusion Detection, Cross-Model Deduplication, and Semantic Memory Tiering - the kernel-level innovations that deliver 4× inference performance improvements.",
  "tags": ["kernel", "tensor-memory", "patent", "performance", "optimization", "deduplication", "numa"],
  "readTimeMinutes": 18,
  "content": "## Introduction: Why Kernel-Level Tensor Memory Management?\n\nTraditional ML frameworks manage tensor memory in userspace, treating the operating system as a black box. This approach has fundamental limitations:\n\n- **No cross-model visibility**: Each model manages memory independently\n- **Redundant allocations**: Identical tensors across models waste memory\n- **NUMA-unaware placement**: Tensors allocated without considering GPU topology\n- **Reactive eviction**: Memory pressure handled after problems occur\n\nMLOS takes a radically different approach: we moved tensor memory management into the Linux kernel itself. This gives us unprecedented visibility and control over ML memory patterns across all running models.\n\nThis blog explores three patented innovations in MLOS's Tensor Memory Manager (TMM), covered under **Patent US-63/865,176: Kernel-Level Optimizations for Machine Learning Workloads**.\n\n---\n\n## Patent Overview: US-63/865,176\n\n**Title:** Kernel-Level Optimizations for Machine Learning Workloads in a Purpose-Built Operating System\n\n**Key Claims:**\n1. ML-Aware Kernel Scheduler\n2. **Tensor Memory Management** ← Focus of this blog\n3. GPU Resource Orchestration\n4. Model Context Switching\n\nThe Tensor Memory Management innovation encompasses three distinct technologies:\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                    MLOS Tensor Memory Manager (TMM)                         │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  ┌─────────────────┐  ┌─────────────────┐  ┌──────────────────────┐        │\n│  │  TENSOR FUSION  │  │  CROSS-MODEL    │  │  SEMANTIC MEMORY     │        │\n│  │   DETECTION     │  │  DEDUPLICATION  │  │      TIERING         │        │\n│  ├─────────────────┤  ├─────────────────┤  ├──────────────────────┤        │\n│  │ Sequential      │  │ SHA-256 hash    │  │ NUMA-aware           │        │\n│  │ access pattern  │  │ content         │  │ GPU-local            │        │\n│  │ analysis        │  │ detection       │  │ allocation           │        │\n│  │                 │  │                 │  │                      │        │\n│  │ Memory reuse    │  │ Copy-on-Write   │  │ Cross-node access    │        │\n│  │ for non-        │  │ semantics for   │  │ tracking for         │        │\n│  │ overlapping     │  │ safe sharing    │  │ migration decisions  │        │\n│  │ tensors         │  │                 │  │                      │        │\n│  └─────────────────┘  └─────────────────┘  └──────────────────────┘        │\n│                                                                             │\n│  Result: 25-40% memory savings across multi-model deployments              │\n└─────────────────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## Innovation 1: Tensor Fusion Detection\n\n### The Problem: Wasteful Sequential Allocation\n\nDuring ML inference, models create many intermediate tensors. In a typical transformer forward pass:\n\n```\n┌──────────────────────────────────────────────────────────────────────────┐\n│                    Traditional Tensor Lifecycle                          │\n├──────────────────────────────────────────────────────────────────────────┤\n│                                                                          │\n│  Time ─────────────────────────────────────────────────────────────────► │\n│                                                                          │\n│  Tensor A: █████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░  (Attention Q)         │\n│  Tensor B: ░░░░░░░░░█████████░░░░░░░░░░░░░░░░░░░  (Attention K)         │\n│  Tensor C: ░░░░░░░░░░░░░░░░░░█████████░░░░░░░░░░  (Attention V)         │\n│  Tensor D: ░░░░░░░░░░░░░░░░░░░░░░░░░░░█████████░  (FFN intermediate)    │\n│                                                                          │\n│  Memory:   ████████████████████████████████████  (All allocated)        │\n│                                                                          │\n│  Total Memory Used: A + B + C + D = 4 × size                            │\n└──────────────────────────────────────────────────────────────────────────┘\n```\n\nThese tensors have **non-overlapping lifetimes**—they're never accessed simultaneously. Traditional systems allocate separate memory for each, wasting precious GPU memory.\n\n### The MLOS Solution: Access Pattern Analysis\n\nMLOS's Tensor Fusion Detection analyzes access patterns at the kernel level:\n\n```c\n/**\n * tensors_can_fuse - Check if two tensors can share memory\n *\n * Criteria:\n * 1. Neither is pinned (can be relocated)\n * 2. Same GPU device\n * 3. Same NUMA node\n * 4. Non-overlapping access patterns (sequential)\n */\nstatic bool tensors_can_fuse(struct mlos_tensor *t1, struct mlos_tensor *t2)\n{\n    /* Can't fuse pinned tensors */\n    if (t1->state == MLOS_TENSOR_PINNED || t2->state == MLOS_TENSOR_PINNED)\n        return false;\n\n    /* Must be on same GPU */\n    if (t1->gpu != t2->gpu)\n        return false;\n\n    /* Must be on same NUMA node */\n    if (t1->numa_node != t2->numa_node)\n        return false;\n\n    /* Check for non-overlapping access (sequential) */\n    if (t1->last_access_ns < t2->last_access_ns &&\n        t1->access_count > 0 && t2->access_count > 0) {\n        return true;  /* t1 completed before t2 started */\n    }\n\n    return false;\n}\n```\n\n### Fusion Groups: Memory Sharing in Action\n\nWhen fusion candidates are identified, they're grouped together:\n\n```\n┌──────────────────────────────────────────────────────────────────────────┐\n│                    MLOS Tensor Fusion Result                             │\n├──────────────────────────────────────────────────────────────────────────┤\n│                                                                          │\n│  Time ─────────────────────────────────────────────────────────────────► │\n│                                                                          │\n│  ┌────────────────── Fused Tensor Block ──────────────────┐              │\n│  │                                                        │              │\n│  │  Tensor A: █████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░      │              │\n│  │  Tensor B: ░░░░░░░░░█████████░░░░░░░░░░░░░░░░░░░      │              │\n│  │  Tensor C: ░░░░░░░░░░░░░░░░░░█████████░░░░░░░░░░      │              │\n│  │  Tensor D: ░░░░░░░░░░░░░░░░░░░░░░░░░░░█████████░      │              │\n│  │                                                        │              │\n│  │            ▼ All share same memory region ▼            │              │\n│  │                                                        │              │\n│  └────────────────────────────────────────────────────────┘              │\n│                                                                          │\n│  Memory:   ████████████░░░░░░░░░░░░░░░░░░░░░░░░░  (max(A,B,C,D))        │\n│                                                                          │\n│  Savings: A + B + C + D - max(A,B,C,D) = 3 × size (75% reduction!)      │\n└──────────────────────────────────────────────────────────────────────────┘\n```\n\n### The Fusion Algorithm\n\n```c\nint mlos_fusion_analyze(struct mlos_model *model, u32 flags,\n                        struct mlos_fusion_group *groups, u32 max_groups,\n                        u32 *num_groups, u64 *total_savings)\n{\n    struct mlos_tensor *tensor, *other;\n    struct mlos_fusion_group *group;\n    u32 group_count = 0;\n    u64 savings = 0;\n\n    /* Iterate through model's tensors */\n    list_for_each_entry(tensor, &model->tensors, model_node) {\n        if (group_count >= max_groups)\n            break;\n\n        /* Start a new potential fusion group */\n        group = &groups[group_count];\n        group->tensor_ids[0] = tensor->id;\n        group->tensor_count = 1;\n        group->combined_size = tensor->size;\n\n        /* Track max tensor size for savings calculation */\n        size_t max_size = tensor->size;\n\n        /* Find other tensors that can fuse with this one */\n        list_for_each_entry(other, &model->tensors, model_node) {\n            if (other == tensor)\n                continue;\n\n            if (tensors_can_fuse(tensor, other)) {\n                group->tensor_ids[group->tensor_count++] = other->id;\n                group->combined_size += other->size;\n                if (other->size > max_size)\n                    max_size = other->size;\n            }\n        }\n\n        /* Calculate savings: total - max(individual sizes) */\n        if (group->tensor_count > 1) {\n            group->saved_size = group->combined_size - max_size;\n            savings += group->saved_size;\n            group_count++;\n        }\n    }\n\n    *num_groups = group_count;\n    *total_savings = savings;\n    return 0;\n}\n```\n\n### Key Innovation: Kernel-Level Visibility\n\nUnlike userspace solutions, MLOS can:\n\n1. **Track access patterns across all models** - not just within one framework\n2. **Use nanosecond-precision timestamps** - `ktime_get_ns()` provides accurate ordering\n3. **Execute fusion atomically** - kernel spinlocks ensure consistency\n4. **Coordinate with memory pressure** - integrate with LRU eviction\n\n---\n\n## Innovation 2: Cross-Model Tensor Deduplication\n\n### The Problem: Duplicate Content Across Models\n\nIn production ML deployments, multiple models often share common components:\n\n```\n┌──────────────────────────────────────────────────────────────────────────┐\n│                    Common Model Components                               │\n├──────────────────────────────────────────────────────────────────────────┤\n│                                                                          │\n│  Model A: BERT-base              Model B: BERT-large (fine-tuned)       │\n│  ┌─────────────────────┐         ┌─────────────────────┐                │\n│  │ Embedding Layer     │    =    │ Embedding Layer     │  ← IDENTICAL  │\n│  │ (128MB)             │         │ (128MB)             │                │\n│  ├─────────────────────┤         ├─────────────────────┤                │\n│  │ Attention Weights   │   ≈     │ Attention Weights   │  ← Similar    │\n│  │ (256MB)             │         │ (512MB)             │                │\n│  ├─────────────────────┤         ├─────────────────────┤                │\n│  │ Output Layer        │    ≠    │ Output Layer        │  ← Different  │\n│  │ (32MB)              │         │ (64MB)              │                │\n│  └─────────────────────┘         └─────────────────────┘                │\n│                                                                          │\n│  Traditional: 416MB + 704MB = 1,120MB                                   │\n│  With Dedup: 416MB + 576MB = 992MB (128MB saved from shared embedding)  │\n└──────────────────────────────────────────────────────────────────────────┘\n```\n\n### The MLOS Solution: Content-Based Deduplication\n\nMLOS implements a SHA-256 hash tree for O(log n) duplicate detection:\n\n```\n┌──────────────────────────────────────────────────────────────────────────┐\n│                    Deduplication Hash Tree                               │\n├──────────────────────────────────────────────────────────────────────────┤\n│                                                                          │\n│                         ┌──────────────┐                                │\n│                         │  Root Node   │                                │\n│                         └──────┬───────┘                                │\n│                    ┌───────────┴───────────┐                            │\n│               ┌────┴────┐             ┌────┴────┐                       │\n│               │ hash<M  │             │ hash>M  │                       │\n│               └────┬────┘             └────┬────┘                       │\n│          ┌─────────┴─────────┐      ┌─────┴─────┐                       │\n│     ┌────┴────┐         ┌────┴────┐ │           │                       │\n│     │sha256_A │         │sha256_B │ │sha256_C   │                       │\n│     │tensor_1 │         │tensor_5 │ │tensor_3   │                       │\n│     │ref_cnt=2│         │ref_cnt=1│ │ref_cnt=3  │                       │\n│     └─────────┘         └─────────┘ └───────────┘                       │\n│         ↑                                   ↑                           │\n│         │                                   │                           │\n│    tensor_7 shares                    tensor_9, tensor_12               │\n│    content with tensor_1              share content with tensor_3       │\n│                                                                          │\n└──────────────────────────────────────────────────────────────────────────┘\n```\n\n### SHA-256 Content Hashing\n\nMLOS uses the Linux kernel's crypto API for efficient hashing:\n\n```c\n/**\n * mlos_dedup_hash_tensor - Compute SHA-256 hash of tensor content\n * @tensor: Tensor to hash\n * @hash_out: Output buffer for 32-byte hash\n *\n * Uses kernel crypto API for hardware-accelerated hashing.\n */\nint mlos_dedup_hash_tensor(struct mlos_tensor *tensor, u8 *hash_out)\n{\n    struct crypto_shash *tfm;\n    struct shash_desc *desc;\n    int ret;\n\n    /* Allocate SHA-256 transform - may use hardware acceleration */\n    tfm = crypto_alloc_shash(\"sha256\", 0, 0);\n    if (IS_ERR(tfm))\n        return PTR_ERR(tfm);\n\n    desc = kmalloc(sizeof(*desc) + crypto_shash_descsize(tfm), GFP_KERNEL);\n    desc->tfm = tfm;\n\n    /* Hash entire tensor content */\n    ret = crypto_shash_digest(desc, tensor->cpu_addr, tensor->size, hash_out);\n\n    kfree(desc);\n    crypto_free_shash(tfm);\n\n    if (ret == 0)\n        atomic64_inc(&mlos_dedup_ctx.tensors_hashed);\n\n    return ret;\n}\n```\n\n### Copy-on-Write (CoW) Semantics\n\nDeduplicated tensors use CoW for safe sharing:\n\n```\n┌──────────────────────────────────────────────────────────────────────────┐\n│                    Copy-on-Write Flow                                    │\n├──────────────────────────────────────────────────────────────────────────┤\n│                                                                          │\n│  Initial State (after dedup):                                           │\n│  ┌─────────────────────────────────────────────────────────────┐        │\n│  │                    Shared Memory Region                      │        │\n│  │                    [Embedding Weights]                       │        │\n│  │                                                              │        │\n│  └─────────────────────────────────────────────────────────────┘        │\n│        ↑                                           ↑                     │\n│        │ read-only                                 │ read-only           │\n│  ┌─────┴─────┐                              ┌─────┴─────┐               │\n│  │  Model A  │                              │  Model B  │               │\n│  │ (ref=1)   │                              │ (ref=1)   │               │\n│  └───────────┘                              └───────────┘               │\n│                                                                          │\n│  After Model B writes (fine-tuning):                                    │\n│  ┌──────────────────────────────┐   ┌──────────────────────────┐        │\n│  │     Original Memory          │   │     New Private Copy     │        │\n│  │    [Embedding Weights]       │   │  [Modified Embeddings]   │        │\n│  └──────────────────────────────┘   └──────────────────────────┘        │\n│        ↑                                           ↑                     │\n│        │ read-only                                 │ read-write          │\n│  ┌─────┴─────┐                              ┌─────┴─────┐               │\n│  │  Model A  │                              │  Model B  │               │\n│  │ (ref=1)   │                              │ (new ref) │               │\n│  └───────────┘                              └───────────┘               │\n│                                                                          │\n│  CoW triggered: Model B gets private copy, original unchanged           │\n└──────────────────────────────────────────────────────────────────────────┘\n```\n\n### Deduplication Entry Structure\n\n```c\n/**\n * struct mlos_dedup_entry - Hash table entry for deduplication\n * @hash: Content hash (SHA-256, 32 bytes)\n * @tensor: Original tensor holding the data\n * @ref_count: Number of tensors sharing this content\n * @rb_node: Red-black tree node for O(log n) lookup\n * @flags: Dedup flags (COW_ENABLED, etc.)\n */\nstruct mlos_dedup_entry {\n    u8 hash[32];\n    struct mlos_tensor *tensor;\n    atomic_t ref_count;\n    struct rb_node rb_node;\n    u32 flags;\n};\n```\n\n### Statistics Tracking\n\nMLOS tracks comprehensive deduplication metrics:\n\n```c\nstruct mlos_dedup_context {\n    struct rb_root hash_tree;           /* O(log n) hash lookup */\n    atomic64_t tensors_hashed;          /* Total tensors analyzed */\n    atomic64_t duplicates_found;        /* Duplicates detected */\n    atomic64_t memory_saved;            /* Total bytes saved */\n    atomic64_t shared_tensors;          /* Currently shared count */\n    atomic64_t hash_collisions;         /* For robustness monitoring */\n    atomic64_t cow_triggers;            /* CoW operations performed */\n    spinlock_t lock;                    /* Thread-safe operations */\n};\n```\n\n---\n\n## Innovation 3: Semantic Memory Tiering (NUMA-Aware Allocation)\n\n### The Problem: NUMA Topology Ignorance\n\nModern servers have Non-Uniform Memory Access (NUMA) topology:\n\n```\n┌──────────────────────────────────────────────────────────────────────────┐\n│                    Typical Multi-Socket Server                           │\n├──────────────────────────────────────────────────────────────────────────┤\n│                                                                          │\n│   NUMA Node 0                              NUMA Node 1                   │\n│  ┌────────────────────────┐              ┌────────────────────────┐     │\n│  │  CPU Socket 0          │              │  CPU Socket 1          │     │\n│  │  ┌──────────────────┐  │              │  ┌──────────────────┐  │     │\n│  │  │  Memory: 256GB   │  │              │  │  Memory: 256GB   │  │     │\n│  │  │  (LOCAL ACCESS)  │  │──────────────│  │  (LOCAL ACCESS)  │  │     │\n│  │  │    ~80ns         │  │   QPI/UPI    │  │    ~80ns         │  │     │\n│  │  └──────────────────┘  │   ~150ns     │  └──────────────────┘  │     │\n│  │                        │   (REMOTE)   │                        │     │\n│  │  PCIe ───────┐         │              │         ┌─── PCIe     │     │\n│  │              │         │              │         │              │     │\n│  │        ┌─────┴─────┐   │              │   ┌─────┴─────┐        │     │\n│  │        │  GPU 0    │   │              │   │  GPU 1    │        │     │\n│  │        │  16GB     │   │              │   │  16GB     │        │     │\n│  │        └───────────┘   │              │   └───────────┘        │     │\n│  └────────────────────────┘              └────────────────────────┘     │\n│                                                                          │\n│  Problem: Tensor for GPU 0 allocated on Node 1 = 2× latency penalty    │\n└──────────────────────────────────────────────────────────────────────────┘\n```\n\n### The MLOS Solution: GPU-Aware NUMA Placement\n\nMLOS automatically places tensors on the NUMA node closest to the target GPU:\n\n```c\n/**\n * mlos_resolve_numa_node - Resolve NUMA node for allocation\n * @gpu: GPU device (used to find local NUMA node)\n * @requested_node: Requested NUMA node (-1 = any, -2 = local)\n *\n * NUMA-aware allocation per patent: Semantic Memory Tiering.\n * Places tensors on NUMA nodes closest to the executing GPU.\n */\nstatic int mlos_resolve_numa_node(struct mlos_gpu_device *gpu, int requested_node)\n{\n    int node;\n\n    if (requested_node == MLOS_NUMA_NODE_ANY) {\n        /* No preference - use current CPU's node */\n        return numa_node_id();\n    }\n\n    if (requested_node == MLOS_NUMA_NODE_LOCAL) {\n        /* Use GPU's local NUMA node if available */\n        if (gpu && gpu->pci_dev) {\n            node = dev_to_node(&gpu->pci_dev->dev);\n            if (node != NUMA_NO_NODE)\n                return node;\n        }\n        /* Fallback to current CPU's node */\n        return numa_node_id();\n    }\n\n    /* Explicit node requested - validate and use */\n    if (requested_node >= 0 && node_online(requested_node)) {\n        return requested_node;\n    }\n\n    return numa_node_id();\n}\n```\n\n### Cross-NUMA Access Tracking\n\nMLOS tracks when tensors are accessed from non-local NUMA nodes:\n\n```c\n/**\n * mlos_tensor_record_access - Record tensor access for NUMA statistics\n * @tensor: Tensor being accessed\n * @accessing_numa_node: NUMA node of the accessing CPU\n *\n * Tracks cross-NUMA node accesses for performance analysis.\n */\nvoid mlos_tensor_record_access(struct mlos_tensor *tensor, int accessing_numa_node)\n{\n    struct mlos_memory_pool *pool = &tensor->gpu->pool;\n    int tensor_node = tensor->numa_node;\n\n    spin_lock_irqsave(&pool->lock, flags);\n\n    /* Record cross-NUMA access */\n    if (tensor_node != accessing_numa_node) {\n        pool->cross_numa_accesses++;\n        pool->numa_stats[tensor_node].remote_accesses++;\n    }\n\n    spin_unlock_irqrestore(&pool->lock, flags);\n}\n```\n\n### Per-Node Statistics\n\n```c\nstruct mlos_numa_pool_stats {\n    int node_id;              /* NUMA node identifier */\n    size_t used_size;         /* Bytes allocated on this node */\n    u32 tensor_count;         /* Number of tensors on this node */\n    u64 allocations;          /* Total allocations */\n    u64 remote_accesses;      /* Accesses from other NUMA nodes */\n    u64 migrations_in;        /* Tensors migrated TO this node */\n    u64 migrations_out;       /* Tensors migrated FROM this node */\n};\n```\n\n### Migration Opportunities\n\nWhen a tensor has high remote access count, MLOS can migrate it:\n\n```\n┌──────────────────────────────────────────────────────────────────────────┐\n│                    NUMA Migration Decision                               │\n├──────────────────────────────────────────────────────────────────────────┤\n│                                                                          │\n│  Tensor X Statistics:                                                    │\n│  ┌────────────────────────────────────────────────────────────┐         │\n│  │  Current NUMA Node: 0                                      │         │\n│  │  Local Accesses: 1,234                                     │         │\n│  │  Remote Accesses: 45,678  ← Anomaly detected!             │         │\n│  │  Remote Access Ratio: 97.4%                                │         │\n│  └────────────────────────────────────────────────────────────┘         │\n│                                                                          │\n│  Decision: Migrate tensor from Node 0 → Node 1                          │\n│                                                                          │\n│  Before:                              After:                             │\n│  ┌───────────┐    ┌───────────┐      ┌───────────┐    ┌───────────┐    │\n│  │  Node 0   │◄───│  Node 1   │      │  Node 0   │    │  Node 1   │    │\n│  │ [Tensor X]│ QPI│  [GPU 1]  │      │           │    │[Tensor X] │    │\n│  └───────────┘    └───────────┘      └───────────┘    └───────────┘    │\n│     150ns latency                        80ns latency (47% faster!)     │\n│                                                                          │\n└──────────────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## The Complete TMM Architecture\n\nAll three innovations work together in the Tensor Memory Manager:\n\n```\n┌──────────────────────────────────────────────────────────────────────────────────┐\n│                         MLOS Tensor Memory Manager                               │\n│                              Architecture                                        │\n├──────────────────────────────────────────────────────────────────────────────────┤\n│                                                                                  │\n│  ┌─────────────────────────────────────────────────────────────────────────────┐ │\n│  │                        Userspace Interface                                  │ │\n│  │  /dev/mlos character device with ioctl + mmap                              │ │\n│  └────────────────────────────────────┬────────────────────────────────────────┘ │\n│                                       │                                          │\n│                                       ▼                                          │\n│  ┌─────────────────────────────────────────────────────────────────────────────┐ │\n│  │                         Kernel TMM Core                                     │ │\n│  │                                                                             │ │\n│  │   ┌─────────────────┐   ┌─────────────────┐   ┌─────────────────┐         │ │\n│  │   │  Memory Pool    │   │  Red-Black Tree │   │  LRU List       │         │ │\n│  │   │  Management     │   │  (O(log n))     │   │  (Eviction)     │         │ │\n│  │   │                 │   │                 │   │                 │         │ │\n│  │   │  total_size     │   │  tensor_tree    │   │  ─────────────► │         │ │\n│  │   │  used_size      │   │  hash_tree      │   │  newest  oldest │         │ │\n│  │   │  fragmented     │   │                 │   │                 │         │ │\n│  │   └─────────────────┘   └─────────────────┘   └─────────────────┘         │ │\n│  │                                                                             │ │\n│  └────────────────────────────────────┬────────────────────────────────────────┘ │\n│                                       │                                          │\n│          ┌────────────────────────────┼────────────────────────────┐            │\n│          │                            │                            │            │\n│          ▼                            ▼                            ▼            │\n│  ┌───────────────────┐   ┌───────────────────┐   ┌───────────────────┐         │\n│  │  Fusion Engine    │   │  Dedup Engine     │   │  NUMA Engine      │         │\n│  ├───────────────────┤   ├───────────────────┤   ├───────────────────┤         │\n│  │ mlos_fusion_      │   │ mlos_dedup_       │   │ mlos_resolve_     │         │\n│  │   analyze()       │   │   hash_tensor()   │   │   numa_node()     │         │\n│  │                   │   │                   │   │                   │         │\n│  │ mlos_fusion_      │   │ mlos_dedup_       │   │ mlos_tensor_      │         │\n│  │   execute()       │   │   enable()        │   │   record_access() │         │\n│  │                   │   │                   │   │                   │         │\n│  │ Access pattern    │   │ SHA-256 hash tree │   │ Cross-node        │         │\n│  │ analysis          │   │ + CoW semantics   │   │ tracking          │         │\n│  └───────────────────┘   └───────────────────┘   └───────────────────┘         │\n│          │                            │                            │            │\n│          └────────────────────────────┼────────────────────────────┘            │\n│                                       │                                          │\n│                                       ▼                                          │\n│  ┌─────────────────────────────────────────────────────────────────────────────┐ │\n│  │                      Hardware Abstraction Layer                             │ │\n│  │                                                                             │ │\n│  │   ┌───────────────┐   ┌───────────────┐   ┌───────────────┐                │ │\n│  │   │  NVIDIA GPU   │   │   AMD ROCm    │   │  Intel oneAPI │                │ │\n│  │   │  via PCI      │   │               │   │               │                │ │\n│  │   └───────────────┘   └───────────────┘   └───────────────┘                │ │\n│  │                                                                             │ │\n│  └─────────────────────────────────────────────────────────────────────────────┘ │\n│                                                                                  │\n└──────────────────────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## Performance Results\n\nOur v7.0.0 benchmarks demonstrate the combined impact:\n\n```\n┌──────────────────────────────────────────────────────────────────────────┐\n│                    Memory Optimization Results                           │\n├──────────────────────────────────────────────────────────────────────────┤\n│                                                                          │\n│  Metric                          Before TMM    After TMM    Improvement │\n│  ────────────────────────────────────────────────────────────────────── │\n│  Memory per model (avg)          2.4 GB        1.8 GB       25% ↓       │\n│  Multi-model memory overhead     +45%          +12%         73% ↓       │\n│  Cross-NUMA accesses             38%           8%           79% ↓       │\n│  Inference latency (P99)         45ms          32ms         29% ↓       │\n│  Max concurrent models (16GB)    6             9            50% ↑       │\n│                                                                          │\n│  Tensor Fusion:                                                         │\n│  - Fusion candidates detected: 847 groups                               │\n│  - Memory saved: 1.2 GB (across 12 models)                             │\n│                                                                          │\n│  Deduplication:                                                         │\n│  - Tensors hashed: 15,234                                               │\n│  - Duplicates found: 2,156 (14.2% dedup ratio)                         │\n│  - Memory saved: 892 MB                                                 │\n│                                                                          │\n│  NUMA Optimization:                                                     │\n│  - Local allocations: 94%                                               │\n│  - Migrations performed: 127                                            │\n│  - Remote access reduction: 79%                                         │\n│                                                                          │\n└──────────────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## Why Kernel-Level Implementation?\n\nThese innovations are only possible at the kernel level:\n\n| Capability | Userspace | Kernel |\n|------------|-----------|--------|\n| Cross-model visibility | ❌ Framework silos | ✅ Global view |\n| Nanosecond timestamps | ❌ Clock overhead | ✅ ktime_get_ns() |\n| Hardware NUMA info | ❌ Indirect access | ✅ dev_to_node() |\n| Zero-copy sharing | ❌ IPC overhead | ✅ Page table sharing |\n| Atomic operations | ❌ Mutex contention | ✅ Spinlocks + atomics |\n| DMA coordination | ❌ Driver roundtrips | ✅ Direct dma_alloc |\n| LRU across models | ❌ Per-process view | ✅ System-wide LRU |\n\n---\n\n## Conclusion\n\nMLOS's Tensor Memory Management represents a fundamental rethinking of how ML systems manage memory. By moving tensor management into the kernel, we achieve:\n\n1. **Tensor Fusion**: 75% memory reduction for sequential tensors\n2. **Cross-Model Dedup**: 14%+ deduplication across model fleet\n3. **NUMA Tiering**: 79% reduction in cross-node memory accesses\n\nThese innovations are protected under **Patent US-63/865,176** and form the foundation of MLOS's performance advantage.\n\n---\n\n## Further Reading\n\n- [MLOS Kernel Architecture Overview](/blog.html?id=mlos-automation-architecture)\n- [Performance Evolution: v6.3.0 to v7.0.0](/blog.html?id=kernel-performance-evolution-v7)\n- [Patent Documentation](https://github.com/mlOS-foundation/patent-docs) (Private)\n\n---\n\n*The MLOS kernel code is available in the [core repository](https://github.com/mlOS-foundation/core) (private) with public releases distributed via [core-releases](https://github.com/mlOS-foundation/core-releases).*"
}
